import { Injectable } from '@angular/core';
import { BehaviorSubject, Observable } from 'rxjs';
import { environment } from '../../environments/environment';

export interface DeepgramTranscript {
  text: string;
  timestamp: Date;
  speaker: 'student' | 'tutor';
  confidence: number;
  isFinal: boolean;
}

export interface DeepgramConnectionStatus {
  connected: boolean;
  transcriptId?: string;
  error?: string;
}

@Injectable({
  providedIn: 'root'
})
export class DeepgramAudioService {
  private ws: WebSocket | null = null;
  private audioContext: AudioContext | null = null;
  private mediaStream: MediaStream | null = null;
  private processor: ScriptProcessorNode | null = null;
  private source: MediaStreamAudioSourceNode | null = null;
  
  // Observables for real-time updates
  private transcriptSubject = new BehaviorSubject<DeepgramTranscript[]>([]);
  public transcript$ = this.transcriptSubject.asObservable();
  
  private connectionStatusSubject = new BehaviorSubject<DeepgramConnectionStatus>({ connected: false });
  public connectionStatus$ = this.connectionStatusSubject.asObservable();
  
  private isRecordingSubject = new BehaviorSubject<boolean>(false);
  public isRecording$ = this.isRecordingSubject.asObservable();
  
  private transcripts: DeepgramTranscript[] = [];
  private currentTranscriptId: string | null = null;
  
  constructor() {}
  
  /**
   * Start real-time transcription with Deepgram
   */
  async startTranscription(lessonId: string, language: string, speaker: 'student' | 'tutor' = 'student'): Promise<void> {
    try {
      console.log('üéôÔ∏è Starting Deepgram real-time transcription...');
      console.log('üéôÔ∏è Lesson:', lessonId, 'Language:', language, 'Speaker:', speaker);
      
      // Clear previous transcripts
      this.transcripts = [];
      this.transcriptSubject.next([]);
      
      // Get microphone access with optimal settings for Deepgram
      this.mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,     // Deepgram prefers 16kHz
          channelCount: 1,       // Mono
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      
      console.log('‚úÖ Got microphone access');
      
      // Set up audio context
      this.audioContext = new AudioContext({ sampleRate: 16000 });
      await this.audioContext.resume(); // Ensure context is running
      
      this.source = this.audioContext.createMediaStreamSource(this.mediaStream);
      
      // Create processor for audio data (4096 samples = ~256ms at 16kHz)
      this.processor = this.audioContext.createScriptProcessor(4096, 1, 1);
      
      console.log('‚úÖ Audio context and processor created');
      
      // Connect to Deepgram WebSocket
      await this.connectWebSocket(lessonId, language, speaker);
      
      // Collect audio data for HTTP upload (every 10 seconds)
      let audioChunks: Float32Array[] = [];
      
      this.processor.onaudioprocess = (event) => {
        const inputBuffer = event.inputBuffer.getChannelData(0);
        
        // Store audio chunks for periodic upload
        audioChunks.push(new Float32Array(inputBuffer));
        
        // Upload every ~10 seconds (assuming 4096 samples at 16kHz = ~256ms per chunk)
        if (audioChunks.length >= 40) { // ~10 seconds of audio
          this.uploadAudioChunks(audioChunks, lessonId, language, speaker);
          audioChunks = []; // Reset for next batch
        }
      };
      
      // Connect audio pipeline
      this.source.connect(this.processor);
      this.processor.connect(this.audioContext.destination);
      
      this.isRecordingSubject.next(true);
      console.log('‚úÖ Deepgram transcription started successfully');
      
    } catch (error) {
      console.error('‚ùå Error starting Deepgram transcription:', error);
      this.connectionStatusSubject.next({ 
        connected: false, 
        error: error instanceof Error ? error.message : 'Unknown error' 
      });
      throw error;
    }
  }
  
  /**
   * Connect to Deepgram via Socket.IO (avoiding WebSocket conflicts)
   */
  private async connectWebSocket(lessonId: string, language: string, speaker: string): Promise<void> {
    return new Promise((resolve, reject) => {
      // For now, let's use a simple HTTP approach to avoid WebSocket conflicts
      // We'll send audio chunks via HTTP POST instead of WebSocket
      console.log('üîå Using HTTP-based Deepgram connection to avoid WebSocket conflicts');
      
      // Simulate connection success
      setTimeout(() => {
        this.connectionStatusSubject.next({ 
          connected: true, 
          transcriptId: lessonId + '_deepgram' 
        });
        console.log('‚úÖ Deepgram HTTP connection ready');
        resolve();
      }, 100);
    });
  }
  
  /**
   * Handle messages from Deepgram WebSocket
   */
  private handleWebSocketMessage(data: any): void {
    switch (data.type) {
      case 'connection':
        if (data.status === 'connected') {
          this.currentTranscriptId = data.transcriptId;
          this.connectionStatusSubject.next({ 
            connected: true, 
            transcriptId: data.transcriptId 
          });
          console.log('‚úÖ Deepgram connection confirmed, transcript ID:', data.transcriptId);
        }
        break;
        
      case 'transcript':
        if (data.text && data.text.trim()) {
          const transcript: DeepgramTranscript = {
            text: data.text.trim(),
            timestamp: new Date(data.timestamp),
            speaker: data.speaker,
            confidence: data.confidence || 0.9,
            isFinal: data.isFinal || true
          };
          
          this.transcripts.push(transcript);
          this.transcriptSubject.next([...this.transcripts]);
          
          console.log(`üéôÔ∏è Deepgram transcribed (${data.speaker}): "${transcript.text}" (confidence: ${transcript.confidence})`);
        }
        break;
        
      case 'error':
        console.error('‚ùå Deepgram error:', data.message, data.error);
        this.connectionStatusSubject.next({ 
          connected: false, 
          error: data.message || 'Deepgram error' 
        });
        break;
        
      default:
        console.log('üîç Unknown Deepgram message type:', data.type, data);
    }
  }
  
  /**
   * Stop transcription and clean up resources
   */
  stopTranscription(): void {
    console.log('üõë Stopping Deepgram transcription...');
    
    // Close WebSocket
    if (this.ws) {
      if (this.ws.readyState === WebSocket.OPEN) {
        this.ws.close(1000, 'Transcription stopped');
      }
      this.ws = null;
    }
    
    // Disconnect audio processing
    if (this.processor) {
      this.processor.disconnect();
      this.processor = null;
    }
    
    if (this.source) {
      this.source.disconnect();
      this.source = null;
    }
    
    if (this.audioContext && this.audioContext.state !== 'closed') {
      this.audioContext.close();
      this.audioContext = null;
    }
    
    // Stop media stream
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => {
        track.stop();
        console.log('üõë Stopped audio track:', track.kind);
      });
      this.mediaStream = null;
    }
    
    // Update observables
    this.isRecordingSubject.next(false);
    this.connectionStatusSubject.next({ connected: false });
    
    console.log('‚úÖ Deepgram transcription stopped and resources cleaned up');
  }
  
  /**
   * Get all transcripts collected so far
   */
  getTranscripts(): DeepgramTranscript[] {
    return [...this.transcripts];
  }
  
  /**
   * Get current transcript ID
   */
  getCurrentTranscriptId(): string | null {
    return this.currentTranscriptId;
  }
  
  /**
   * Clear all transcripts
   */
  clearTranscripts(): void {
    this.transcripts = [];
    this.transcriptSubject.next([]);
    console.log('üóëÔ∏è Cleared all transcripts');
  }
  
  /**
   * Get connection status
   */
  getConnectionStatus(): DeepgramConnectionStatus {
    return this.connectionStatusSubject.value;
  }
  
  /**
   * Check if currently recording
   */
  isRecording(): boolean {
    return this.isRecordingSubject.value;
  }
  
  /**
   * Upload audio chunks via HTTP (fallback when WebSocket isn't available)
   */
  private async uploadAudioChunks(chunks: Float32Array[], lessonId: string, language: string, speaker: string): Promise<void> {
    try {
      console.log(`üì§ Uploading ${chunks.length} audio chunks via HTTP...`);
      
      // Combine all chunks into one buffer
      const totalLength = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
      const combinedBuffer = new Float32Array(totalLength);
      let offset = 0;
      
      for (const chunk of chunks) {
        combinedBuffer.set(chunk, offset);
        offset += chunk.length;
      }
      
      // Convert to WAV format for better compatibility
      const wavBuffer = this.floatArrayToWav(combinedBuffer, 16000);
      const audioBlob = new Blob([wavBuffer], { type: 'audio/wav' });
      
      console.log(`üì¶ Created WAV blob: ${audioBlob.size} bytes`);
      
      // Upload via existing transcription service
      const formData = new FormData();
      formData.append('audio', audioBlob, 'audio.wav');
      formData.append('speaker', speaker);
      
      // Use existing HTTP endpoint (but we need to start transcription first)
      const response = await fetch(`${environment.backendUrl}/api/transcription/${this.currentTranscriptId}/audio`, {
        method: 'POST',
        body: formData,
        headers: {
          'Authorization': `Bearer dev-token-phillip-dacosta` // TODO: Get real token
        }
      });
      
      if (response.ok) {
        const result = await response.json();
        console.log('‚úÖ Audio uploaded successfully:', result);
        
        // Simulate transcript received
        if (result.text) {
          const transcript: DeepgramTranscript = {
            text: result.text,
            timestamp: new Date(),
            speaker: speaker as 'student' | 'tutor',
            confidence: 0.9,
            isFinal: true
          };
          
          this.transcripts.push(transcript);
          this.transcriptSubject.next([...this.transcripts]);
        }
      } else {
        console.error('‚ùå Audio upload failed:', response.status, response.statusText);
      }
      
    } catch (error) {
      console.error('‚ùå Error uploading audio chunks:', error);
    }
  }
  
  /**
   * Convert Float32Array to WAV format
   */
  private floatArrayToWav(buffer: Float32Array, sampleRate: number): ArrayBuffer {
    const length = buffer.length;
    const arrayBuffer = new ArrayBuffer(44 + length * 2);
    const view = new DataView(arrayBuffer);
    
    // WAV header
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    };
    
    writeString(0, 'RIFF');
    view.setUint32(4, 36 + length * 2, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, 1, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);
    writeString(36, 'data');
    view.setUint32(40, length * 2, true);
    
    // Convert float samples to 16-bit PCM
    let offset = 44;
    for (let i = 0; i < length; i++) {
      const sample = Math.max(-1, Math.min(1, buffer[i]));
      view.setInt16(offset, sample * 0x7FFF, true);
      offset += 2;
    }
    
    return arrayBuffer;
  }
}
